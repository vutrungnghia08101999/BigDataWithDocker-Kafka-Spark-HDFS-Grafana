{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext                                                                                        \n",
    "from pyspark.sql import SparkSession                                                                                    \n",
    "from pyspark.streaming import StreamingContext                                                                          \n",
    "from pyspark.streaming.kafka import KafkaUtils    \n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'Spark-kafka-streaming.ipynb', 'spark-streaming-kafka-0-8-assembly_2.11-2.4.1.jar']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.Builder() \\\n",
    "     .appName(\"SparkStreamingKafka\") \\\n",
    "     .master(\"spark://streaming-spark-master:7077\") \\\n",
    "     .config('spark.jars.packages', 'org.apache.spark:spark-streaming-kafka-0-8-assembly_2.11:2.4.1') \\\n",
    "     .config(\"spark.jars\", \"spark-streaming-kafka-0-8-assembly_2.11-2.4.1.jar\")\n",
    "     .config(\"spark.driver.allowMultipleContexts\", \"true\") \\\n",
    "     .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "     .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/hive\") \\\n",
    "     .enableHiveSupport() \\\n",
    "     .getOrCreate()\n",
    "\n",
    "# ss = SparkSession.Builder() \\\n",
    "#      .appName(\"TEST\") \\\n",
    "#      .master(\"spark://spark-master:7077\") \\\n",
    "#      .enableHiveSupport() \\\n",
    "#      .getOrCreate()\n",
    "# #\n",
    "#org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1\n",
    "#org.apache.spark:spark-streaming-kafka-0-8:2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ss.sparkContext\n",
    "ssc = StreamingContext(sc, 5)\n",
    "ss.sparkContext.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_rdd(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        global ss\n",
    "        print(f\"Number of records: {len(rdd.collect())}\")\n",
    "        df = ss.createDataFrame(\n",
    "            rdd,\n",
    "            schema=[\n",
    "                'ArrivalTime',\n",
    "                'BusinessLeisure',\n",
    "                'CabinCategory',\n",
    "                'CreationDate',\n",
    "                'CurrencyCode',\n",
    "                'DepartureTime',\n",
    "                'Destination',\n",
    "                'OfficeIdCountry',\n",
    "                'Origin',\n",
    "                'TotalAmount',\n",
    "                'nPAX'\n",
    "            ])\n",
    "        df.write.saveAsTable(name='default.trips', format='hive', mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_list(s):\n",
    "    t = json.loads(s)\n",
    "    results = []\n",
    "    for k, v in t.items():\n",
    "        results.append(v)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 447\n",
      "Number of records: 384\n",
      "Number of records: 324\n",
      "Number of records: 404\n",
      "Number of records: 1259\n",
      "Number of records: 3120\n",
      "Number of records: 3360\n",
      "Number of records: 3280\n",
      "Number of records: 3383\n",
      "Number of records: 3435\n",
      "Number of records: 3503\n",
      "Number of records: 3275\n",
      "Number of records: 3318\n",
      "Number of records: 3337\n",
      "Number of records: 3524\n",
      "Number of records: 3241\n",
      "Number of records: 3296\n",
      "Number of records: 3330\n",
      "Number of records: 3364\n",
      "Number of records: 3317\n",
      "Number of records: 3466\n",
      "Number of records: 3533\n",
      "Number of records: 3549\n",
      "Number of records: 3460\n",
      "Number of records: 3505\n",
      "Number of records: 3574\n",
      "Number of records: 3257\n",
      "Number of records: 3388\n",
      "Number of records: 3373\n",
      "Number of records: 3172\n",
      "Number of records: 2988\n",
      "Number of records: 3456\n",
      "Number of records: 3326\n",
      "Number of records: 3563\n",
      "Number of records: 3131\n",
      "Number of records: 1791\n",
      "Number of records: 2969\n",
      "Number of records: 3355\n",
      "Number of records: 3543\n",
      "Number of records: 3544\n",
      "Number of records: 3471\n",
      "Number of records: 3407\n",
      "Number of records: 3368\n",
      "Number of records: 3366\n",
      "Number of records: 3261\n",
      "Number of records: 2956\n",
      "Number of records: 2962\n",
      "Number of records: 3003\n",
      "Number of records: 3016\n",
      "Number of records: 3302\n",
      "Number of records: 3445\n",
      "Number of records: 3226\n",
      "Number of records: 3237\n",
      "Number of records: 3308\n",
      "Number of records: 3393\n",
      "Number of records: 3339\n",
      "Number of records: 3348\n",
      "Number of records: 3101\n"
     ]
    }
   ],
   "source": [
    "ks = KafkaUtils.createDirectStream(\n",
    "    ssc, ['trips'], {'metadata.broker.list': 'kafka-broker-1:9093,kafka-broker-2:9093'})\n",
    "lines = ks.map(lambda x: x[1])\n",
    "\n",
    "# transform = lines.map(lambda tweet: (tweet, int(len(tweet.split())), int(len(tweet))))\n",
    "transform = lines.map(lambda tripInfo: json_to_list(tripInfo))\n",
    "transform.foreachRDD(handle_rdd)\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
